{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e95e268",
   "metadata": {},
   "source": [
    "# FinEmo-LoRA: Initial Project Work\n",
    "## CSCI 4/6366 Intro to Deep Learning - Deliverable II\n",
    "\n",
    "**Project:** Fine-Grained Financial Emotion Classification  \n",
    "**Date:** November 21, 2025  \n",
    "**Course:** CSCI 6366 Neural Networks and Deep Learning  \n",
    "**Instructor:** Professor Joel Klein\n",
    "\n",
    "---\n",
    "\n",
    "## Team Information\n",
    "\n",
    "**Team Member:**\n",
    "- **Vaishnavi Kamdi** - GitHub: [@vaish725](https://github.com/vaish725/FinEmo-LoRA.git)\n",
    "\n",
    "---\n",
    "\n",
    "## Project Summary\n",
    "\n",
    "This project develops a deep learning system for fine-grained emotion classification in financial text. Unlike traditional sentiment analysis (positive/negative/neutral), we classify text into **6 economic emotions**: anxiety, excitement, optimism, fear, uncertainty, and hope.\n",
    "\n",
    "**Approach:**\n",
    "1. **Current:** Lightweight feature-based classifier (DistilBERT + MLP)\n",
    "2. **Future:** LoRA fine-tuning of Llama 3.1 8B with two-stage training\n",
    "\n",
    "**Key Innovation:** Using GPT-4 for high-quality pseudo-labeling of financial text with confidence scoring and automated quality control.\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset Sources\n",
    "\n",
    "1. **FinGPT Sentiment Dataset**\n",
    "   - Source: https://github.com/FinancialDiets/FINGPT\n",
    "   - License: Apache 2.0\n",
    "   - Description: Financial news headlines and social media posts\n",
    "\n",
    "2. **GoEmotions Dataset**\n",
    "   - Source: https://github.com/google-research/google-research/tree/master/goemotions\n",
    "   - Paper: https://arxiv.org/abs/2005.00547\n",
    "   - License: Apache 2.0\n",
    "   - Description: 58k Reddit comments with 27 emotion labels (for transfer learning)\n",
    "\n",
    "3. **SEntFiN Dataset**\n",
    "   - Source: https://www.kaggle.com/datasets/ankurzing/sentiment-analysis-for-financial-news\n",
    "   - License: CC BY-SA 4.0\n",
    "   - Description: Entity-aware financial sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ae8a14",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35d0513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path('/Users/vaishnavikamdi/Documents/GWU/Classes/Fall 2025/NNDL/FinEmo-LoRA')\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "print(f\"✓ Project root: {project_root}\")\n",
    "print(f\"✓ Python version: {sys.version.split()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d296be7",
   "metadata": {},
   "source": [
    "## 2. Dataset Exploration\n",
    "\n",
    "Load and explore the annotated financial emotion dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc1115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned annotated dataset\n",
    "data_path = project_root / 'data' / 'annotated' / 'fingpt_annotated_v2.csv'\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117172f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion class distribution\n",
    "print(\"=\"*80)\n",
    "print(\"EMOTION DISTRIBUTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "emotion_counts = df['emotion'].value_counts().sort_index()\n",
    "print(\"\\nClass counts:\")\n",
    "for emotion, count in emotion_counts.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"  {emotion:<15} {count:>4} ({percentage:>5.1f}%)\")\n",
    "\n",
    "print(f\"\\nTotal samples: {len(df)}\")\n",
    "print(f\"Class imbalance ratio: {emotion_counts.max() / emotion_counts.min():.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0ad02a",
   "metadata": {},
   "source": [
    "## 3. Data Visualization\n",
    "\n",
    "Visualize emotion distribution and sample text characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b29d605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize emotion distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar plot\n",
    "emotion_counts.plot(kind='bar', ax=axes[0], color='steelblue', alpha=0.8)\n",
    "axes[0].set_title('Emotion Class Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Emotion', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(emotion_counts, labels=emotion_counts.index, autopct='%1.1f%%',\n",
    "            startangle=90, colors=sns.color_palette('pastel'))\n",
    "axes[1].set_title('Emotion Distribution (%)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Visualization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffac6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze text length distribution\n",
    "df['text_length'] = df['text'].str.len()\n",
    "df['word_count'] = df['text'].str.split().str.len()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Text length distribution\n",
    "axes[0].hist(df['text_length'], bins=30, color='coral', alpha=0.7, edgecolor='black')\n",
    "axes[0].axvline(df['text_length'].mean(), color='red', linestyle='--', \n",
    "                linewidth=2, label=f'Mean: {df[\"text_length\"].mean():.0f}')\n",
    "axes[0].set_title('Distribution of Text Length (Characters)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Character Count', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Word count distribution\n",
    "axes[1].hist(df['word_count'], bins=30, color='mediumseagreen', alpha=0.7, edgecolor='black')\n",
    "axes[1].axvline(df['word_count'].mean(), color='red', linestyle='--',\n",
    "                linewidth=2, label=f'Mean: {df[\"word_count\"].mean():.0f}')\n",
    "axes[1].set_title('Distribution of Word Count', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Word Count', fontsize=12)\n",
    "axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nText Statistics:\")\n",
    "print(f\"  Avg characters: {df['text_length'].mean():.0f} (std: {df['text_length'].std():.0f})\")\n",
    "print(f\"  Avg words: {df['word_count'].mean():.0f} (std: {df['word_count'].std():.0f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc41dc04",
   "metadata": {},
   "source": [
    "## 4. Sample Annotations\n",
    "\n",
    "Examine sample annotations from GPT-4 with confidence scores and reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646988b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample annotations for each emotion\n",
    "print(\"=\"*80)\n",
    "print(\"SAMPLE ANNOTATIONS (One per emotion)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for emotion in sorted(df['emotion'].unique()):\n",
    "    sample = df[df['emotion'] == emotion].iloc[0]\n",
    "    print(f\"\\n[{emotion.upper()}]\")\n",
    "    print(f\"Text: {sample['text'][:150]}...\")\n",
    "    if 'confidence' in df.columns:\n",
    "        print(f\"Confidence: {sample['confidence']}\")\n",
    "    if 'reasoning' in df.columns:\n",
    "        print(f\"Reasoning: {sample['reasoning'][:120]}...\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2b7b4b",
   "metadata": {},
   "source": [
    "## 5. Baseline Model: Feature-Based Classifier\n",
    "\n",
    "Our current baseline uses a **lightweight feature-based approach**:\n",
    "1. **Feature Extraction:** DistilBERT (frozen, pre-trained) generates 768-dimensional embeddings\n",
    "2. **Classifier:** 3-layer MLP (768 → 512 → 256 → 128 → 6 classes)\n",
    "3. **Training:** Cross-entropy loss with class weights to handle imbalance\n",
    "\n",
    "This approach is:\n",
    "- ✅ **Fast**: Training takes ~2 minutes on CPU\n",
    "- ✅ **CPU-friendly**: No GPU required\n",
    "- ✅ **Interpretable**: Can analyze feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e477316",
   "metadata": {},
   "source": [
    "### Install Required Packages\n",
    "\n",
    "First, let's ensure all required packages are installed in the notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3058dfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = ['xgboost', 'scikit-learn', 'torch']\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"✓ {package} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "        print(f\"✓ {package} installed\")\n",
    "\n",
    "print(\"\\n✓ All required packages are available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70cdbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained classifier\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Add scripts to path for unpickling\n",
    "import sys\n",
    "scripts_path = str(project_root / 'scripts')\n",
    "if scripts_path not in sys.path:\n",
    "    sys.path.insert(0, scripts_path)\n",
    "\n",
    "# Define MLP architecture (must match training)\n",
    "class MLPClassifierPyTorch(nn.Module):\n",
    "    def __init__(self, input_dim=768, hidden_layers=[512, 256, 128], num_classes=6, dropout=0.3):\n",
    "        super(MLPClassifierPyTorch, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_layers:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        layers.append(nn.Linear(prev_dim, num_classes))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Load the most recent trained model\n",
    "model_dir = project_root / 'models' / 'classifiers'\n",
    "model_files = sorted(model_dir.glob('mlp_*.pkl'))\n",
    "\n",
    "if model_files:\n",
    "    latest_model = model_files[-1]\n",
    "    print(f\"Loading model: {latest_model.name}\")\n",
    "    \n",
    "    # Import the classifier module to help with unpickling\n",
    "    try:\n",
    "        from classifier import train_classifier\n",
    "    except ImportError:\n",
    "        pass  # OK if module not found, we've defined the class above\n",
    "    \n",
    "    with open(latest_model, 'rb') as f:\n",
    "        checkpoint = pickle.load(f)\n",
    "    \n",
    "    classifier = checkpoint['model']\n",
    "    label_encoder = checkpoint['label_encoder']\n",
    "    \n",
    "    print(f\"✓ Model loaded successfully\")\n",
    "    print(f\"  Feature dimension: {checkpoint['feature_dim']}\")\n",
    "    print(f\"  Classes: {list(label_encoder.classes_)}\")\n",
    "    print(f\"  Model type: {checkpoint['classifier_type']}\")\n",
    "    \n",
    "    # Display model architecture\n",
    "    if isinstance(classifier, MLPClassifierPyTorch):\n",
    "        print(f\"\\nModel architecture:\")\n",
    "        print(classifier)\n",
    "else:\n",
    "    print(\"⚠️  No trained model found. Please run training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbea5a7f",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation Results\n",
    "\n",
    "Load and visualize the latest evaluation results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bbab80",
   "metadata": {},
   "source": [
    "### Alternative: View Results from JSON\n",
    "\n",
    "If the model pickle loading has issues, we can view the evaluation results directly from saved metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b888dc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation results from JSON (if available)\n",
    "import json\n",
    "\n",
    "results_dir = project_root / 'results'\n",
    "\n",
    "# Check for metrics files directly in results directory\n",
    "metrics_files = sorted(results_dir.glob('evaluation_metrics_*.json'))\n",
    "\n",
    "if metrics_files:\n",
    "    latest_metrics = metrics_files[-1]\n",
    "    print(f\"Loading results from: {latest_metrics.name}\\n\")\n",
    "    \n",
    "    with open(latest_metrics, 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nOverall Metrics:\")\n",
    "    print(f\"  Accuracy: {metrics['overall_accuracy']:.4f}\")\n",
    "    print(f\"  Macro Precision: {metrics['macro_precision']:.4f}\")\n",
    "    print(f\"  Macro Recall: {metrics['macro_recall']:.4f}\")\n",
    "    print(f\"  Macro F1: {metrics['macro_f1']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nPer-Class Performance:\")\n",
    "    print(f\"{'Emotion':<15} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<10}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for emotion, class_metrics in metrics['per_class_metrics'].items():\n",
    "        print(f\"{emotion:<15} \"\n",
    "              f\"{class_metrics['precision']:<12.4f} \"\n",
    "              f\"{class_metrics['recall']:<12.4f} \"\n",
    "              f\"{class_metrics['f1_score']:<12.4f} \"\n",
    "              f\"{class_metrics['support']:<10}\")\n",
    "    \n",
    "    print(\"\\n✓ Results loaded successfully\")\n",
    "    print(f\"\\nTimestamp: {metrics['timestamp']}\")\n",
    "else:\n",
    "    print(\"⚠️  No evaluation results found. Run evaluation first:\")\n",
    "    print(\"    python run_pipeline.py --stage evaluate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de484e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix from latest evaluation\n",
    "from PIL import Image\n",
    "\n",
    "# Check both locations for confusion matrices\n",
    "confusion_matrix_paths = [\n",
    "    project_root / 'results',\n",
    "    project_root / 'models' / 'classifiers'\n",
    "]\n",
    "\n",
    "cm_files = []\n",
    "for path in confusion_matrix_paths:\n",
    "    cm_files.extend(path.glob('confusion_matrix_*.png'))\n",
    "\n",
    "cm_files = sorted(cm_files)\n",
    "\n",
    "if cm_files:\n",
    "    latest_cm = cm_files[-1]\n",
    "    print(f\"Latest confusion matrix: {latest_cm.name}\")\n",
    "    print(f\"Location: {latest_cm.parent.name}/\\n\")\n",
    "    \n",
    "    img = Image.open(latest_cm)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title('Confusion Matrix - Latest Model', fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"⚠️  No confusion matrix found. Run evaluation first:\")\n",
    "    print(\"    python run_pipeline.py --stage evaluate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e511028d",
   "metadata": {},
   "source": [
    "## 7. Initial Results & Observations\n",
    "\n",
    "### Current Performance (Baseline Model)\n",
    "\n",
    "**Original Dataset (200 samples):**\n",
    "- Accuracy: 63.33%\n",
    "- Macro F1: 0.33\n",
    "- **Challenge:** Model never predicted excitement, optimism, or fear (0% F1 for these classes)\n",
    "\n",
    "**Key Finding:** Error analysis revealed **data quality issues**:\n",
    "- 60-70% of \"optimism\" labels were neutral factual statements\n",
    "- Example: *\"Company acquires €420M in assets\"* (factual, not emotional)\n",
    "- GPT-4 confused business-positive context with emotional optimism\n",
    "\n",
    "### Data Cleaning Effort\n",
    "\n",
    "Implemented automated annotation review with sentiment heuristics:\n",
    "- Cleaned 83 \"optimism\" samples → 54 true optimism + 18 relabeled to uncertainty\n",
    "- Reduced dataset to 189 samples with higher quality labels\n",
    "\n",
    "**Cleaned Dataset (189 samples):**\n",
    "- Accuracy: 44.74% (initial retrain)\n",
    "- Macro F1: 0.39\n",
    "- **Observation:** Performance dropped due to smaller dataset size\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Short-term:**\n",
    "   - Consider 3-class taxonomy (negative/positive/uncertainty) for small dataset\n",
    "   - Scale up to 500+ annotated samples\n",
    "   - Experiment with data augmentation\n",
    "\n",
    "2. **Long-term:**\n",
    "   - Implement LoRA fine-tuning pipeline\n",
    "   - Two-stage training: GoEmotions (58k samples) → Financial domain\n",
    "   - Target: 75-85% accuracy with balanced per-class performance\n",
    "\n",
    "### Challenges Identified\n",
    "\n",
    "1. **Small Dataset:** 189 samples insufficient for 6-class classification\n",
    "2. **Class Imbalance:** 12.7x ratio (uncertainty: 76, hope: 6)\n",
    "3. **Annotation Quality:** Automated annotation requires careful quality control\n",
    "4. **Rare Classes:** Hope, fear, and excitement have very few samples (<20 each)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e2f1c4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary for Deliverable II\n",
    "\n",
    "### Completed Work\n",
    "\n",
    "✅ **Data Pipeline**\n",
    "- Downloaded and preprocessed FinGPT dataset (200+ samples)\n",
    "- Implemented data loaders and preprocessing utilities\n",
    "\n",
    "✅ **Annotation System**\n",
    "- GPT-4-based emotion annotation with confidence scoring\n",
    "- Automated quality control with sentiment heuristics\n",
    "- Generated 189 high-quality annotated samples\n",
    "\n",
    "✅ **Baseline Model**\n",
    "- Feature-based classifier (DistilBERT + MLP)\n",
    "- CPU-friendly implementation\n",
    "- Achieved 63.33% accuracy on initial data\n",
    "\n",
    "✅ **Error Analysis**\n",
    "- Identified annotation quality issues\n",
    "- Implemented data cleaning pipeline\n",
    "- Documented challenges and next steps\n",
    "\n",
    "### Repository Contents\n",
    "\n",
    "- `README.md` - Complete project documentation\n",
    "- `scripts/` - Python modules for data, training, and evaluation\n",
    "- `notebooks/` - This notebook demonstrating initial work\n",
    "- `config.yaml` - Central configuration file\n",
    "- `data/` - Datasets and annotations\n",
    "- `models/` - Trained classifiers\n",
    "\n",
    "### GitHub Repository\n",
    "\n",
    "**URL:** https://github.com/vaish725/FinEmo-LoRA\n",
    "\n",
    "**Team:** Vaishnavi Kamdi ([@vaish725](https://github.com/vaish725))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
