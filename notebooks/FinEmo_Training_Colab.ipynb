{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c76c2c6",
   "metadata": {},
   "source": [
    "# FinEmo Training on Google Colab\n",
    "\n",
    "**Why Colab?** PyTorch on macOS has segfault bugs with BatchNorm/weighted loss. Colab provides free GPU access with no issues.\n",
    "\n",
    "**Expected Accuracy**: 60-70% (vs 46% on macOS CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5cbcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install Dependencies\n",
    "!pip install torch torchvision xgboost scikit-learn pandas numpy matplotlib seaborn -q\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85cc437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Upload Data Files\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "os.makedirs('data/annotated', exist_ok=True)\n",
    "os.makedirs('data/features', exist_ok=True)\n",
    "os.makedirs('models/classifiers', exist_ok=True)\n",
    "\n",
    "print(\"Please upload: fingpt_annotated_scaled.csv\")\n",
    "uploaded = files.upload()\n",
    "!mv fingpt_annotated_scaled.csv data/annotated/\n",
    "\n",
    "print(\"\\nPlease upload: train_features_scaled.npy\")\n",
    "uploaded = files.upload()\n",
    "!mv train_features_scaled.npy data/features/\n",
    "\n",
    "print(\"\\n✅ Files uploaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90698d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define Model Architecture\n",
    "import torch.nn as nn\n",
    "\n",
    "class ImprovedMLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=768, hidden_dims=[512, 384, 256, 128], \n",
    "                 num_classes=6, dropout=0.4):\n",
    "        super(ImprovedMLPClassifier, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))  # Works on GPU!\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, num_classes))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "print(\"✅ Model class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e433a9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Load and Prepare Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LOADING DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load features and labels\n",
    "X = np.load('data/features/train_features_scaled.npy')\n",
    "df = pd.read_csv('data/annotated/fingpt_annotated_scaled.csv')\n",
    "y = df['emotion'].values\n",
    "\n",
    "print(f\"\\nDataset: {len(X)} samples, {X.shape[1]} features\")\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(f\"Classes: {list(label_encoder.classes_)}\")\n",
    "\n",
    "# Show distribution\n",
    "print(\"\\nClass distribution:\")\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"  {label:<15} {count:>3} ({count/len(y)*100:.1f}%)\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain: {len(X_train)}, Val: {len(X_val)}\")\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced', classes=np.unique(y_train), y=y_train\n",
    ")\n",
    "\n",
    "print(\"\\nClass weights (for imbalance handling):\")\n",
    "for i, weight in enumerate(class_weights):\n",
    "    print(f\"  {label_encoder.classes_[i]:<15} {weight:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61be776f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Balance Dataset with SMOTE + Setup Focal Loss\n",
    "!pip install imbalanced-learn -q\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Apply SMOTE to balance classes\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BALANCING DATASET WITH SMOTE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nOriginal distribution:\")\n",
    "for i, emotion in enumerate(label_encoder.classes_):\n",
    "    count = (y_train == i).sum()\n",
    "    print(f\"  {emotion:<15} {count} samples\")\n",
    "\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42, k_neighbors=3)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"\\nOriginal: {len(X_train)} samples\")\n",
    "print(f\"After SMOTE: {len(X_train_balanced)} samples\")\n",
    "\n",
    "print(\"\\nBalanced distribution:\")\n",
    "for i, emotion in enumerate(label_encoder.classes_):\n",
    "    count = (y_train_balanced == i).sum()\n",
    "    print(f\"  {emotion:<15} {count} samples\")\n",
    "\n",
    "# Define Focal Loss (better for imbalanced data)\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        \n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha[targets]\n",
    "            focal_loss = alpha_t * focal_loss\n",
    "            \n",
    "        return focal_loss.mean()\n",
    "\n",
    "# Recompute class weights for balanced data\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights_balanced = compute_class_weight(\n",
    "    'balanced', classes=np.unique(y_train_balanced), y=y_train_balanced\n",
    ")\n",
    "class_weights_tensor = torch.FloatTensor(class_weights_balanced).to(device)\n",
    "\n",
    "# Convert balanced data to tensors\n",
    "X_train_t = torch.FloatTensor(X_train_balanced).to(device)\n",
    "y_train_t = torch.LongTensor(y_train_balanced).to(device)\n",
    "X_val_t = torch.FloatTensor(X_val).to(device)\n",
    "y_val_t = torch.LongTensor(y_val).to(device)\n",
    "\n",
    "# Create data loader\n",
    "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize model with REDUCED dropout\n",
    "model = ImprovedMLPClassifier(\n",
    "    input_dim=X_train.shape[1],\n",
    "    hidden_dims=[512, 384, 256, 128],\n",
    "    num_classes=len(label_encoder.classes_),\n",
    "    dropout=0.25  # Reduced from 0.4\n",
    ").to(device)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL ARCHITECTURE\")\n",
    "print(\"=\"*80)\n",
    "print(model)\n",
    "print(f\"\\nTrainable parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Use Focal Loss and better hyperparameters\n",
    "criterion = FocalLoss(alpha=class_weights_tensor, gamma=2.0)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Ready to train with SMOTE + Focal Loss!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0bc775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Train Model with Data Augmentation\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "epochs = 150  # Increased from 100\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "patience = 20  # Increased from 15\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        # Add Gaussian noise for data augmentation (10% of batches)\n",
    "        if np.random.rand() < 0.1:\n",
    "            noise = torch.randn_like(batch_X) * 0.01\n",
    "            batch_X = batch_X + noise\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val_t)\n",
    "        val_loss = criterion(val_outputs, y_val_t).item()\n",
    "        _, predicted = torch.max(val_outputs, 1)\n",
    "        val_acc = (predicted == y_val_t).float().mean().item()\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "              f\"Train Loss: {train_loss:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, \"\n",
    "              f\"Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        best_model_state = model.state_dict().copy()\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(best_model_state)\n",
    "print(f\"\\n✅ Training complete! Best val loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ecaa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Evaluate Model\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_outputs = model(X_val_t)\n",
    "    _, y_pred = torch.max(val_outputs, 1)\n",
    "    y_pred = y_pred.cpu().numpy()\n",
    "\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    y_val, y_pred, average='macro', zero_division=0\n",
    ")\n",
    "\n",
    "print(f\"\\nOverall Metrics:\")\n",
    "print(f\"  Accuracy:  {accuracy:.4f} ({accuracy*100:.1f}%)\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1-Score:  {f1:.4f}\")\n",
    "\n",
    "print(\"\\nPer-Class Metrics:\")\n",
    "print(classification_report(\n",
    "    y_val, y_pred,\n",
    "    target_names=label_encoder.classes_,\n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "# Check if target achieved\n",
    "target_met = accuracy >= 0.75\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Target Accuracy: 75-80%\")\n",
    "print(f\"Achieved:        {accuracy*100:.1f}%\")\n",
    "print(f\"Status:          {'✅ TARGET ACHIEVED!' if target_met else '⚠️ Below target (may need more data)'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc6edb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Save and Download Model\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "model = model.cpu()\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_path = f'models/classifiers/mlp_improved_colab_{timestamp}.pkl'\n",
    "\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'model': model,\n",
    "        'label_encoder': label_encoder,\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1,\n",
    "        'trained_on': 'Google Colab GPU',\n",
    "        'architecture': 'ImprovedMLP [768->512->384->256->128->6]',\n",
    "        'timestamp': timestamp\n",
    "    }, f)\n",
    "\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "print(\"\\nDownloading model...\")\n",
    "\n",
    "files.download(model_path)\n",
    "print(\"\\n✅ Done! Model downloaded to your computer.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
