
# FinEmo-LoRA unified requirements
# Combined dependencies from requirements.txt and requirements_logits.txt
# Target: Python 3.10+ (project used 3.13 venv locally)

# Core Deep Learning Framework
torch>=2.1.0
transformers>=4.35.0
accelerate>=0.24.0
peft>=0.6.0  # Parameter-Efficient Fine-Tuning (LoRA)
bitsandbytes>=0.41.0  # For 4-bit quantization (QLoRA)

# Model / Dataset ecosystem
datasets>=2.14.0
huggingface-hub>=0.17.0

# Data Processing
pandas>=2.0.0
numpy>=1.24.0
scikit-learn>=1.3.0

# Machine Learning Classifiers
xgboost>=2.0.0

# LLM API Integration for Annotation
openai>=1.3.0  # For GPT-4 based annotation
anthropic>=0.5.0  # Optional: Claude API
python-dotenv>=1.0.0  # For API key management

# Visualization and Evaluation
matplotlib>=3.7.0
seaborn>=0.12.0
plotly>=5.17.0

# Configuration Management
pyyaml>=6.0
omegaconf>=2.3.0

# Progress Tracking
tqdm>=4.65.0

# Kaggle API (for optional SEntFiN dataset download)
kaggle>=1.5.16

# Logging and Monitoring
wandb>=0.15.0  # Weights & Biases for experiment tracking (optional)
tensorboard>=2.14.0

# Utilities / Notebooks
jupyter>=1.0.0
ipykernel>=6.25.0
python-dateutil>=2.8.2

# Testing and Quality
pytest>=7.4.0
black>=23.7.0  # Code formatting

# Notes:
# - If you plan to use quantized inference with 4-bit weights, keep `bitsandbytes` and
#   `accelerate` installed and configure `device_map` appropriately.
# - Depending on the LLM and tokenizer you use, you may also need `sentencepiece` or
#   other tokenizer backends; install those as-needed.

