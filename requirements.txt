# FinEmo-LoRA Requirements
# Python 3.10+ recommended

# Core Deep Learning Framework
torch>=2.1.0
transformers>=4.35.0
accelerate>=0.24.0
peft>=0.6.0  # Parameter-Efficient Fine-Tuning (LoRA)
bitsandbytes>=0.41.0  # For 4-bit quantization (QLoRA)

# Dataset Management
datasets>=2.14.0
huggingface-hub>=0.17.0

# Data Processing
pandas>=2.0.0
numpy>=1.24.0
scikit-learn>=1.3.0

# LLM API Integration for Annotation
openai>=1.3.0  # For GPT-4 based annotation
anthropic>=0.5.0  # Alternative: Claude API
python-dotenv>=1.0.0  # For API key management

# Visualization and Evaluation
matplotlib>=3.7.0
seaborn>=0.12.0
plotly>=5.17.0

# Configuration Management
pyyaml>=6.0
omegaconf>=2.3.0

# Progress Tracking
tqdm>=4.65.0

# Kaggle API (for SEntFiN dataset download)
kaggle>=1.5.16

# Logging and Monitoring
wandb>=0.15.0  # Weights & Biases for experiment tracking (optional)
tensorboard>=2.14.0

# Utilities
jupyter>=1.0.0
ipykernel>=6.25.0
python-dateutil>=2.8.2

# Optional: Fast inference optimization
# unsloth @ git+https://github.com/unslothai/unsloth.git  # Uncomment for faster Llama training

# Testing and Quality
pytest>=7.4.0
black>=23.7.0  # Code formatting

